name: CI testing

# see: https://help.github.com/en/actions/reference/events-that-trigger-workflows
on: # Trigger the workflow on push or pull request, but only for the main branch
  push:
    branches: [main]
  pull_request:
    branches: [main]
    types: [opened, reopened, ready_for_review, synchronize]

defaults:
  run:
    shell: bash

jobs:
  pytester:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: ["ubuntu-22.04", "macos-14", "windows-2022"]
        python-version: ["3.9", "3.10", "3.11", "3.12", "3.13"]

    # Timeout: https://stackoverflow.com/a/59076067/4521646
    timeout-minutes: 60
    env:
      TORCH_URL: "https://download.pytorch.org/whl/cpu/torch_stable.html"

    steps:
      - uses: actions/checkout@v4
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: "pip"

      - name: Install package & dependencies
        run: |
          pip --version
          pip install -U lightning-sdk
          pip install -e ".[extras]" -r requirements/test.txt -U -q --find-links $TORCH_URL
          pip list

      - name: Tests
        run: |
          # Set environment variables for Python 3.12+ to improve multiprocessing behavior
          if [[ "${{ matrix.python-version }}" == "3.12" || "${{ matrix.python-version }}" == "3.13" ]]; then
            export PYTHONUNBUFFERED=1
            export DATA_OPTIMIZER_TIMEOUT=20
            # Force spawn method for multiprocessing to avoid hanging issues
            export MULTIPROCESSING_START_METHOD=spawn
            # Disable fork safety warnings for cleaner output
            export PYTHONNOFORK=1
            # Enable more detailed output for debugging
            export PYTEST_LOG_LEVEL=DEBUG
            # Collect thread dumps on test failures
            export PYTEST_PRINT_THREADS_ON_FAILURE=1
            # Run tests with enhanced debugging for Python 3.12+
            coverage run --source litdata -m pytest tests -v --tb=short -x --durations=0 --log-cli-level=DEBUG
          else
            # Run tests normally for other Python versions
            coverage run --source litdata -m pytest tests -v
          fi

      - name: Statistics
        run: |
          coverage report
          coverage xml

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          file: ./coverage.xml
          flags: unittests,${{ matrix.os }},${{ matrix.python-version }}
          env_vars: OS,PYTHON
          name: codecov-umbrella
          fail_ci_if_error: false

  testing-guardian:
    runs-on: ubuntu-latest
    needs: pytester
    if: always()
    steps:
      - run: echo "${{ needs.pytester.result }}"
      - name: failing...
        if: needs.pytester.result == 'failure'
        run: exit 1
      - name: cancelled or skipped...
        if: contains(fromJSON('["cancelled", "skipped"]'), needs.pytester.result)
        timeout-minutes: 1
        run: sleep 90
